import { GoogleGenAI } from "@google/genai";
import { desc, eq } from "drizzle-orm";
import type { RuntimeConfig } from "@config";
import { createDb } from "@db/client";
import { messages } from "@db/schema";
import type { AppBindings } from "@appTypes/bindings";

export async function embedAndStoreMessages(
  env: AppBindings,
  config: RuntimeConfig,
  messagesArray: { id: string; channelId: string; authorUsername: string; content: string }[],
) {
  if (messagesArray.length === 0) return;
  const ai = new GoogleGenAI({ apiKey: config.GEMINI_API_KEY });

  try {
    const MAX_BATCH_GEMINI = 100;
    for (let i = 0; i < messagesArray.length; i += MAX_BATCH_GEMINI) {
      const batchMessages = messagesArray.slice(i, i + MAX_BATCH_GEMINI);
      const contents = batchMessages.map(msg => `[Membro: ${msg.authorUsername}]: ${msg.content || ""}`);

      const response = await ai.models.embedContent({
        model: "models/gemini-embedding-001",
        contents,
        config: { outputDimensionality: 768 },
      });

      const embeddings = response.embeddings;
      if (embeddings && embeddings.length > 0) {
        const vectorsToInsert = batchMessages.map((msg, index) => {
          const values = embeddings[index]?.values;
          if (!values) return null;
          return {
            id: msg.id,
            values,
            namespace: msg.channelId,
            metadata: {
              authorUsername: msg.authorUsername,
              content: msg.content,
            },
          };
        }).filter(v => v !== null) as any[];

        if (vectorsToInsert.length > 0) {
          console.log(`[Vectorize] Salvando ${vectorsToInsert.length} mem√≥rias no namespace ${batchMessages[0].channelId}. Dim: ${vectorsToInsert[0].values.length}`);
          await env.VECTORIZE.upsert(vectorsToInsert);
          console.log(`[Vectorize] Salvas ${vectorsToInsert.length} mem√≥rias! (Chunk ${i / MAX_BATCH_GEMINI + 1})`);
        }
      }
    }
  } catch (err) {
    console.error(`[Vectorize] Erro ao incorporar:`, err);
  }
}

export async function generateBotResponse(
  env: AppBindings,
  config: RuntimeConfig,
  userPrompt: string,
  isAskCommand: boolean = false,
  channelId?: string,
): Promise<string> {
  // 1. Inicializar o Gemini API
  const ai = new GoogleGenAI({ apiKey: config.GEMINI_API_KEY });

  // 2. Criar Embedding da Pergunta Atual para Busca (RAG)
  let vectorMatches: Array<any> = [];
  let chatContext = "Nenhum hist√≥rico passado recente com esse assunto...";

  try {
    const embeddingRes = await ai.models.embedContent({
      model: "models/gemini-embedding-001",
      contents: userPrompt,
      config: { outputDimensionality: 768 },
    });

    const searchVector = embeddingRes.embeddings?.[0]?.values;

    const targetNamespace = channelId || config.DISCORD_CHANNEL_ID;

    // 3. Buscar as 10 mem√≥rias matem√°ticas mais relevantes no Vectorize (filtradas pelo namespace do Canal atual)
    if (searchVector && targetNamespace) {
      console.log(`[Vectorize] Buscando mem√≥rias no namespace: ${targetNamespace}. Query dim: ${searchVector.length}`);
      const queryResult = await env.VECTORIZE.query(searchVector, {
        topK: 10,
        namespace: targetNamespace,
        returnMetadata: "all",
      });

      if (queryResult.matches && queryResult.matches.length > 0) {
        vectorMatches = queryResult.matches;
        console.log(`[Vectorize] Encontradas ${vectorMatches.length} mem√≥rias relevantes!`);
      } else {
        console.log(`[Vectorize] Nenhuma mem√≥ria encontrada no namespace ${targetNamespace}.`);
      }
    }

    if (vectorMatches.length > 0) {
      chatContext = vectorMatches
        .map((m) => `[${m.metadata?.authorUsername}]: ${m.metadata?.content}`)
        .join("\n");
    }
  } catch (err) {
    console.error("[Vectorize] Falha na busca por RAG:", err);
  }

  const systemPrompt = isAskCommand
    ? `Voc√™ √© o "El Matadore", um membro de um grupo de amigos no Discord que acabou de ser invocado com o comando /ask para responder a uma pergunta de forma aprofundada.
Leia as Mem√≥rias Relevantes passadas (recuperadas via busca sem√¢ntica) para entender se voc√™s j√° debateram isso antes ou pegar contextos valiosos. D√™ uma resposta COMPLETA, PROFUNDA e T√âCNICA (se for o caso), N√ÉO limite seu conhecimento ou resposta de "thinking". No entanto, aja naturalmente como membro da turma, misturando genialidade t√©cnica com a zoeira e o tom do grupo.
    
--- MEM√ìRIAS RELEVANTES DO CHAT ---
${chatContext}
---------------------------------

Responda a pergunta do usu√°rio a seguir com toda a sua capacidade:`
    : `Voc√™ √© um membro engra√ßado de um grupo de amigos no Discord (chamado "El Matadore"). 
N√£o aja como um assistente de IA engessado. Abaixo est√£o algumas Mem√≥rias Relevantes e semelhantes do grupo que a Busca Sem√¢ntica encontrou. D√™ uma resposta direta, sem rodeios e natural.
    
--- MEM√ìRIAS RELEVANTES DO CHAT ---
${chatContext}
---------------------------------

Use esse contexto se fizer sentido. Agora responda a √∫ltima mensagem (onde mencionaram voc√™)!`;

  const promptText = `${systemPrompt}\n\nNova marca√ß√£o/pergunta para voc√™ responder:\n${userPrompt}`;

  try {
    const response = await ai.models.generateContent({
      model: "models/gemini-3-flash-preview", // Pode usar "gemini-2.5-flash" se preferir a estabilidade
      contents: [{ role: "user", parts: [{ text: promptText }] }],
      config: isAskCommand ? {} : {},
    });

    return response.text ?? "Fiquei sem palavras! ü§ê";
  } catch (error) {
    console.error("Erro no Gemini:", error);
    return "Deu pane no meu sistema, rapaziada! ü§ñüî•";
  }
}
